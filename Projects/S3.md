[[AWS]] [[Boto3]]
## Buckets
- basic container within s3 used for stroage of objects
	- referred to with an Amazon Resource Name (ARN)
- upload as many objects as you need (unlimited storage)
- can create up to 100 buckets by default (can add more by requesting service limit increase)
- buckets must be created in a region
- subresources define how the bucket is configured
	- subresource: resource that belongs to another resource and cannot exist on its own
-  **s3 has a universal namespace**: buckets must be unique globally regardless of the region they are created
	- can be accessedd via Virtual style URL
	  - virtual: `http://bucket.s3.amazon-aws.com`, `http://bucket.s3-aws-region.amazonaws.com`
	    
## Region
- the geographical location where s3 will store the buckets you create
- objects stored in a region will never leave it (unless you explicitly transfer them out)
- choose a region to minimize latency, costs, or to adress a particular compliance requirement

## Objects
- **s3 is a key:value store designed to store an unlimited number of objects**
- objects consist of:
	- key = name of the object
	- value = the data being stored (0-5TB)
	- Version ID = string of data assigned to an object when veursioning is enabled
**Bucket + Key + Version ID uniquely identifies an object in S3**
- Metadata = name+value pairs which are used to store information about the object
- subresources = additional resources specifically assigned to an object
- access control information = policies for controlling access to the resource
- S3 has a **flat file structure**
	- has no directories (unlike a file system)
	- however, directories can be imitated by the use of *prefixes*
![[Pasted image 20220502112244.png]]
- object key names can use UTF-8 encoding, but cannot be longer than 1024 bytes
- when naming, it's recommended to use DNS-safe naming + characters
	- [0-9A-Za-z] and !,-,_,., *, ',(,)

#### Object tagging
- allows you to categorize objects using a key:value pair, i.e.:
	- project=acloudguru
	- classification=confidential
- object tags enable:
	- fine-grained access control
	- fine-grained lifecycle management
	- filtering for CloudWatch metrics and Cloudtrail Logs
- object tagging features
	- keys can be 128 unicode chars in length
	- values can be 256 unicode chars in length
	- keys & values are case sensitive
	- up to 10 tags per object
	- each tag must have a unique key
### Consistency Model
- S3 provides **strong consistency** for puts, lists, and deletes of **objects**
	- after any of these operations are performed, any subsequent read request immediately recieves the latest version of the object
	- note that *bucket operations are eventually consistent* 
	- s3 does not provide object locking - if 2 requests are made around the same time, the one with the **latest timestamp wins**
### Security
- designed for secure, primary, "mission critical" data providing 11x9's durability & 99.99% availability
	- performs checksums on data and repairs corruption using redundant data
	- cross-region replication provides the ability to improve durability by optionally replicating data across multiple regions
- versioning, if enabled, provides the ability to retrieve every version of every object ever stored in S3, even if the object is deleted
- s3 objects are **secure by default**
	- only the bucket & object owners have access to the resources they create
	- permissions to objects & buckets are granted using **policies**
	- all access to s3 resources can be optionally logged to provide audit trails

### Making requests
- s3 is a RESTful web service
	- interact with it over web based protocols (http & https)
	- requests made via the REST API
- interactions usually take place via the following (that effectively all wrap around the REST API):
	- AWS management console
	- AWS CLI
	- AWS SDKs (e.g. boto3)

### Costs
- utility-based pricing (only pay for what you use)
	- charges for:
		- storage size
		- number of requests
		- data egress
		- transfer acceleration
		- management functions
			- monitoring metrics (some are free)
			- storage class analysis
			- s3 inventory
		- object tagging

## Object storage classes
- s3 provides different tiers of storage for your objects based on need
	- largely depends on access requirements, performance, and your specific use case
- Objects in s3 use the `STANDARD` storage class by default
	- specify storage class upon object's creation
	- can also change storage class of existing objects in s3
### Storage class types
#### Frequently accessed objects:
##### `STANDARD`: the default storage class
- highly durable (via replication)
- highly available - millisecond access to data
##### `REDUCED_REDUNDANCY`
- similar to `STANDARD` , but with less durability because data is not replicated as many times
- designed for **non-critical, reproducible data**
- NOT RECOMMENDED as it comes at a higher cost than STANDARD (don't use this)
#### infrequently accessed objects
##### `STANDARD_IA` (infrequent access)
- highly durable
- highly available
- millisecond latency
- minimum billable object size of 128 kb & 30-day retention time
- **lower fees than `STANDARD`, but you are charged a retrieval/revival fee**
##### `ONEZONE_IA`
- highly durable with millisecond latency
- minimum billable object size of 128KB and 30 day retention time with a revival fee
- data is only stored in a single availability zone and therefore is less available than `STANDARD_IA`, however the storage fees are also lower

#### Rarely accessed Objects
##### `GLACIER`
- highly durable, cheap storage class designed for archive data where a portion needs to be retrievable in minutes/hours
- has a minimum billable storage time of 90 days
- data is not available in real-time, and instead must be restored
##### `DEEP_ARCHIVE`
- highly durable, **extremely cheap** storage class designed for archival or rarely accessed data
- has a minimum billable storage time of 180 days
- data is not available in real-time and instead can be restored within hours/days

##### `INTELLIGENT_TIERING`
- is a storage class that automatically moves data between frequent and infrequent access tiers
	- objects that have not been accessed for 30 days are moved from frequent to infrequent access
- helps to optimize storage costs
	- note: no retrieval/revival fee is charged for objects accessed in the infrequent access tiers (there is a small charge per object for movement into this tier however)


## Uploading to s3
### upload interfaces
- Console
-  AWS CLI
- AWS SDKs (like boto3)
### Transfer Acceleration
- use **CloudFront** (global content distribution network) for download
- use **Transfer Acceleration** for ingest
	- enabled **per bucket**
	- `bucketname.s3-accelerate.amazonaws.com`
	- `bucketname.s3-accelerate.dualstack.amazonaws.com` for IPV6
	- Additional costs:
		- $0.04/GB for US, Europe, Japan
		- $0.08/GB for all other AWS Edge locations

#### Multipart Uploads
- single S3 `PUT` can be up to 5GiB
- S3 Objects can be up to 5TiB with multipart upload
	- prepare data: break data down into reasonably sized pieces
	- move pieces into s3
	- s3 puts it back together
- 3 API Calls:
1. `CreateMultipartUpload` API call: returns `Bucket`, `Key`, and `UploadID`
2. `UploadPart` API call: provide `Bucket`, `Key` `Part Number`, & `UploadID`. Returns `ETag`
3. `CompleteMultipartUpload` API call: provide `Bucket`, `Key` `Part Number`, `UploadID` , & all `part number`s & `ETag`s
- can be made up of up to 10,000 parts
- specifying the same part number as a previously uploaded part will overwrite that part
- auto-abort policy can be specified to abort multipart upload after a specified time period
##### considerations
- utilize multipart upload for files > 100MiB
- all parts except final part must be at least 5Mib
	- files should be between 5-100 MiB
##### `boto3` funcs
```python
import argparse
import boto3
import json
import multiproccessing

def start_upload(bucket, key):
	s3_client = boto3.client('s3')

	response = s3_client.create_multipart_upload(
		Bucket = bucket,
		Key = key)

	return response['UploadId']

def add_upload_part(proc_queue, body, bucket, key, part_number, upload_id):
	s3_client = boto3.client('s3')

	response = s3_client.upload_part(
		Body = body,
		Bucket = bucket,
		Key = key,
		PartNumber = part_number,
		UploadId = upload_id)

	print(f'Finished part: {part_number}, ETag: {response['ETag']}')
	proc_queue.put({'PartNumber': part_number, 'ETag': response['ETag']})
	return None

def end_upload(bucket, key, upload_id, finished_parts):
	s3_client = boto3.client('s3')

	response = s3_client.complete_multipart_upload(
		Bucket = bucket,
		Key = key,
		MultipartUpload={'Parts': finished_parts},
		UploadId = upload_id)

def main():
	ap = argparse.ArgumentParser()
	ap.add_argument('-f', '--file', required = True, help = 'file to be chunked & uploaded')
	...
	args = vars(ap.pars_args())

	if args['key'] in [None, '']:
		args['key'] = args['file']

	file = args['file']
	key = args['key']
	...
	file_upload = open(file, 'rb')
	part_proc = []
	proc_queue = multiprocessing.Queue()
	queue_returns = []
	chunk_size = (args['chunk_size'] * 1024) * 1024
	part_num
```


# S3 Storage classes
##### Design
- *11 9's durability* (99.999999999%)
- 2-4 9's availability (99.5%-99.99%)
#### SLA
- 2-3 nines availability (99%-99.9%)
### S3 Standard
 - general purpose, frequent access
 -  primary data store
 - high throughput, low latency
 - often the first stop in data storage lifecycle
 - 3 9s availability
 - $0.023 per GB
### S3 Intelligent-Tiering
- unknown access patterns
- access patterns monitored
- if a standard s3 object is not accessed for 30 days, it will be moved to the configured infrequent access tier
- if it is requested, it will be moved to s3 standard
### S3 Infrequent Access
- weekly to monthly access
- $0.01 per 1,000 write requests
- $0.01 per GB retrieval
- objects smaller than 128Kb are billed as 128Kb objects - better to combine small objects if possible
- $0.0125 per GB stored
- 2 9s availability SLA
- Also, **one zone infrequet access**
	- easily replaced data (more fragile in s3)
	- $0.01 per GB stored
### S3 Glacier
- archive, acceptable minutes to hours retrieval
- $0.004/GB storage
| Data Retrieval | 1000 requests | Per GB  |
| -------------- | ------------- | ------- |
| Expedited      | $10$          | $0.03   |
| Standard       | $0.05         | $0.01   |
| Bulk           | $0.025        | $0.0025 |
| Capacity Unit  | n/a           | $100    |

#### S3 Glacier Deep Archive
- archive, acceptable hours retrieval (within 12 hours)
- good for compliance purposes
- 3 9's SLA
- **lowest storage cost of s3: $0.00099/GB

| Data Retrieval | 1000 requests | Per GB  |
| -------------- | ------------- | ------- |
| Expedited      | $0.10         | $0.02   |
| Standard       | $0.025         | $0.0025   |
| Bulk           | $0.025        | $0.0025 |
| Capacity Unit  | n/a           | $100    |


# S3 Lifecycle Policies

